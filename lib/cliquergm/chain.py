import numpy as np
from itertools import zip_longest
from cliquergm.plotstyles import plot_imports
import cliquergm.sample_tools as st
import cliquergm.statistic as statistic

"""Chain class provides a platform for sampling an initial graph."""


class Chain(object):
    """Chain object provides a platform for sampling an initial graph.

    Attributes
    ----------
    current_graph : cliquergm.Graph
        Working graph, on which proposal changes are made during sampling.

    sample_list : array
        Array of :class:`~cliquergm.graph.Graph` objects.
        Each sampled graph is stored as its own graph object in the chain's
        sample list.

    model : cliquergm.model.Model
        A pointer to the model object to which the chain belongs. Used for
        access to model-global attributes such as lock and sample_preferences.

    size : integer
        Number of nodes in ``current_graph`` (used as reference, not changed).

    randomstate : numpy random state
        (Default ``None``)
        For chains which belong to a Model object, this will be initialized
        when each chain is created by Model.__init__ function. Its purpose is
        to preserve RNG state during multiprocessing calls, and will be managed
        by the implementation of multiprocessing in model.py.
    """

    def __init__(self, initial_graph, stats, model):
        """Initialize a chain with initial graph, statistics, and model pointer.

        Parameters
        ----------
        initial_graph : cliquergm.Graph
            Initial graph object generated by an cliquergm graph generator
            function.

        stats : dict
            Dictionary of statistic parameters keyed by statistic subclass
            names.

        model : cliquergm.Model
            A pointer to a model object to which the chain belongs.
        """
        self.randomstate = None
        self.initial_graph = initial_graph.copy()
        self.current_graph = initial_graph.copy()
        self.current_graph.add_stats(stats)
        self.model = model  # should this have a different name?
        # Model is oddly referenced (for lock and preferences)...

        self.sample_list = []
        self.size = initial_graph.number_of_nodes

    def _replacewith(self, newchain):
        """Makes self exactly like newchain, but without changing model pointer
        information or initial_graph. Assumes graph size remains the same.
        This is to facilitate concurrent sampling with multiprocessing."""
        self.current_graph = newchain.current_graph
        self.sample_list = newchain.sample_list

    def sample(self, samples):
        """Sample chain current graph using current statistic parameters.

        Parameters
        ----------
        samples : int
            Number of samples to be collected.

        Notes
        -----
        Sample function iterates the current graph, saving samples to
        sample_list using model ``"burnin"`` and ``"sample_interval"``
        attributes. Model ``"output"`` attribute determines how sampling
        progress is displayed. Options are ``"None"``, ``"Progress"``,
        ``"Terminal"``, and ``"File"``. These preferences can be adjusted
        with the Model.update_sample_preferences() function.
        """
        chain_name = "Chain {}".format(self.model.chains.index(self) + 1)
        output = self.model.sample_preferences['output']
        sampling_output = chain_name + ":\n"
        # Burnin:
        if output != "None":
            print("Burnin for: ", chain_name)
        for i in range(self.model.sample_preferences['burnin']):
            self.current_graph.iterate()

        # Intro Printing:
        if output == "Progress":
            print(self.current_graph.print_intro())
        elif output != "None":
            sampling_output += self.current_graph.print_intro()

        # Sample:
        if output != "None":
            print("Sampling for:", chain_name)
        for i in range(samples):
            for j in range(self.model.sample_preferences['sample_interval']):
                self.current_graph.iterate()
            if output == "Progress":
                print(self.current_graph.print_summary())
                # print(self.current_graph.print_debug_summary())
            elif output != "None":
                sampling_output += self.current_graph.print_summary()
                # sampling_output += self.current_graph.print_debug_summary()
            self.sample_list.append(self.current_graph.copy())
        # Conclusion printing:
        line = self.current_graph.print_summary()
        line_length = len(line) + 6 * line.count("\t")
        if output == "Progress":
            print("-" * 60)
        elif output != "None":
            sampling_output += "-" * line_length
            if output == "Terminal":
                if self.model.lock:
                    self.model.lock.acquire()
                    try:
                        print(sampling_output)
                    finally:
                        self.model.lock.release()
                else:
                    print(sampling_output)
            elif output == "File":
                filename = 'logs/SampleFrom{}.txt'.format(
                    chain_name)
                with open(filename, mode='w+', encoding='utf-8') as f:
                    f.write(sampling_output)

        if self.model.lock:
            # If concurrent sampling, must return self to preserve changes to
            # chain in each process's memory. This is a workaround.
            return(self)

    def fit(self, target_counts, a):
        """Fit statistic parameters to achieve target subgraph distributions.

        Currently, this method replaces the chain's current_graph parameters
        with the fitted parameters.

        Parameters
        ----------
        target_counts : dict
            Dictionary of target counts, keyed by names of subclasses of the
            :class:`~cliquergm.statistic.Statistic` class. The keys in this
            dictionary must match exactly the keys in the
            ``self.current_graph.stats`` dictionary. For subclasses which
            require a vector of counts, the vector passed should be exactly the
            right length. That is, the clique count vector must match the
            number of nodes in the graphs being sampled.

        a : float
            Initial gain value for the algorithm. Choose ``a`` to have a value
            of about ``0.1`` if the initial parameters are expected to be far
            from the fitted parameter values, and ``0.01`` if the initial
            parameter values are expected to be close to the fitted values.

        Returns
        -------
        fit_stats : dict
            Dictionary of fitted parameters, keyed by names of subclasses of
            the :class:`~cliquergm.statistic.Statistic` class.
        """
        # Convert target_counts dictionary to a vector in the right order.
        C = np.array([], dtype=float)
        for key in statistic.names:  # for consistent ordering
            if key in target_counts:
                C = np.append(C, target_counts[key])

        p = len(self.current_graph.parameters())

        # First part of process
        N1 = p * 3
        self.sample(N1)

        def compute_D_matrix(lastsamples):
            D = np.zeros((p, p))
            u_bar = np.array([0.0] * p)
            complement_list = []
            for G in self.sample_list[-lastsamples:]:
                complement_list.append(G.copy())
                complement_list[-1].invert()
                complement_list[-1].update_stat_counts()

            pr_list = [0] * lastsamples
            for i in range(1, 1 + lastsamples):
                pr_list[-i] = min(1, (complement_list[-i].probability()
                                      / self.sample_list[-i].probability()))
            for i in range(1, 1 + lastsamples):
                D += pr_list[-i] * st.outersquare(complement_list[-i].counts())
                D += ((1 - pr_list[-i])
                      * st.outersquare(self.sample_list[-i].counts()))
                u_bar += (pr_list[-i] * complement_list[-i].counts()
                          + (1 - pr_list[-i]) * self.sample_list[-i].counts())
            u_bar = u_bar / lastsamples
            D = D/lastsamples + st.outersquare(u_bar)  # Make sure correct
            return(D)

        D = compute_D_matrix(N1)
        # D is not in general invertible, if a count is always zero among the
        # sampled graphs. try adding 1's along diagonal:
        for i in range(p):
            if D[i, i] < 0.001:
                D[i, i] = 1
        D0inv = np.linalg.inv(np.diag(np.diag(D)))

        # Second part of process
        for n in range(1, 5):
            Ln = int(2**((n-1)/3) * (7+p))
            Un = Ln + 200
            theta = [self.current_graph.parameters()]
            Zsum = [0] * p
            Zlast = [0] * p
            for m in range(Un):
                stats = self.current_graph.parameter_dict()
                self.current_graph = self.initial_graph.copy()
                self.current_graph.add_stats(stats)
                self.sample(1)
                G = self.sample_list[-1]
                Ginv = G.copy()
                Ginv.invert()
                prG = min(1, G.probability() / G.probability())
                Z = prG * Ginv.counts() + (1-prG) * G.counts() - C
                theta.append(theta[-1] - a * np.matmul(D0inv, Z))
                Zsum += Zlast * Z
                Zlast = Z

                if m >= Ln and max(Zsum) < 0:
                    break
            self.current_graph._update_parameter_vect(np.mean(theta, axis=0))

        # Third part of the process
        print("The estimation process yielded parameters:")
        fitted_dict = self.current_graph.parameter_dict()
        for key in statistic.names:  # for order matching w/below printed vects
            if key in fitted_dict:
                print(key, ": ", fitted_dict[key])
        print()

        self.sample(1000)
        print("Parameter estimate standard error vector",
              "(Same order as parameters above, concatenated):\n",
              np.diag(compute_D_matrix(1000)))

        counts = np.zeros((1000, p))
        for i in range(1, 1001):
            counts[i-1] = self.sample_list[-i].counts()

        tk = (np.mean(counts, axis=0) - C) / np.std(counts, axis=0)
        print()
        print("t ratio vector (where |t_k| <= 0.3 indicates acceptable",
              "convergence to target count distribution):")
        print(tk, "\n")
        print("Mean count vector:")
        print(np.mean(counts, axis=0), "\n")
        return(fitted_dict)

    def prune_samples(self, n):
        """Keep only the last n samples in sample_list."""
        self.sample_list = self.sample_list[-n:]

    def pick_samples(self, n, start=None):
        """Randomly pick `n` samples from sample_array, starting at index ``start``,
        where start is a negative index.
        """
        if start is None:
            start = -len(self.sample_list)
        ind = np.random.choice(np.arange(start, 0), n, replace=False)
        ind.sort()
        self.sample_list = [self.sample_list[i] for i in ind]

    def counts(self):
        """Summarize statistic counts of stored samples.

        Returns
        -------
        count_array : numpy array
            Parameter counts are organized as ``count_array[t,i]`` where ``t``
            is time and ``i`` is the parameter.
        """
        return(np.array([graph.counts()
                         for graph in self.sample_list]))

    def count_dict(self, split=False):
        """Summarize statistic counts of stored samples in a dictionary.

        Returns
        -------
        count_dict : dictionary {str: numpy array}
            Parameter counts are keyed by statistic name, with arrays
            of counts as values.
        """
        return(st.count_dict(self.sample_list, split=split))

    def avg_count_dict(self):
        return(st.avg_count_dict(self.sample_list))

    def avg_density(self):
        return(st.avg_density(self.sample_list))

    def nonzero_counts(self):
        """Summarize 'significant' statistic counts of stored samples.

        Method discards parameters whose counts are close to zero, as
        determined by _is_zero function via identify_zero_counts
        function.

        Returns
        -------
        count_array : numpy array
            Parameter counts are organized as count_array[t,i] where t is time
            and i is the parameter.
        """
        count_array = self.counts()
        delete_array = identify_zero_counts(count_array)
        count_array = np.delete(count_array, delete_array, 1)
        return(count_array)

    @plot_imports
    def plot_data(self, show=True, save=False, figures=None, modules=None, **kwargs):
        pl, ps, ff = modules
        imgsize = kwargs.pop('imgsize')

        count_dict = self.count_dict(split=False)
        count_dictsplit = self.count_dict(split=True)
        count_dictsplit = {k: v for k, v in count_dictsplit.items()
                           if not _is_zero(v)}

        samples = len(self.sample_list)
        aggregate_degreedist = np.array([0] * self.size)
        snapshot_cliquedist = []
        snapshot_density = []
        snapshot_diameter = []
        for g in self.sample_list:
            aggregate_degreedist = np.array(list(map(sum, zip_longest(
                aggregate_degreedist, g.degree_histogram(), fillvalue=0))))
            snapshot_density.append(g.density())
            if g.is_connected():
                snapshot_diameter.append(g.diameter())
        aggregate_degreedist = aggregate_degreedist / (samples * self.size)

        if 'Cliques' in count_dict:
            snapshot_cliquedist = count_dict['Cliques']
        else:
            snapshot_cliquedist = []
            for g in self.sample_list:
                snapshot_cliquedist.append(st.dist(g.find_cliques(), self.size))

        aggregate_cliquedist = np.mean(snapshot_cliquedist, axis=0)

        # Find size of largest clique observed in all samples
        max_clique_size = len(st.trim_zeros(aggregate_cliquedist))

        figs = []
        # Plot Mean Maximal Clique Size histogram
        x = [str(i) for i in range(1, max_clique_size + 1)]
        y = aggregate_cliquedist[:max_clique_size]
        y = y / sum(y)  # normalize
        cliquebar = ps.barplot(x=x, y=y)
        figs.append(pl.graph_objs.Figure(data=[cliquebar], layout=ps.baselayout()))
        figs[-1].layout.xaxis.update(title="Maximal Clique Size")
        figs[-1].layout.yaxis.update(title="Frequency")

        #  Node Degree Frequency in Sampled Graphs
        titles = ("Maximal Clique Size Frequency", "Node Degree Distribution")
        maxnodedegree = len(st.trim_zeros(aggregate_degreedist))
        x = [str(i) for i in range(1, maxnodedegree + 1)]
        y = aggregate_degreedist / sum(aggregate_degreedist)
        degreebar = ps.barplot(x=x, y=y[:maxnodedegree])
        figs.append(pl.graph_objs.Figure(data=[degreebar], layout=ps.baselayout()))
        figs[-1].layout.xaxis.update(title="Node Degree")
        figs[-1].layout.yaxis.update(title="Frequency")
        figs[-1].layout.update(showlegend=False)

        # Plot Convergence / stability of parameter counts in samples
        titles = ("Parameter Counts", "Graph Density")
        fig2 = ps.tworowfig(titles=titles, shared_xaxes=True)

        x = [g.iteration for g in self.sample_list]
        for key in count_dictsplit:
            fig2.append_trace(pl.graph_objs.Scatter(x=x, y=count_dictsplit[key],
                                                    name=key), 1, 1)
        fig2.layout.xaxis1.update(title="Graph Iteration")
        fig2.layout.yaxis1.update(title="Parameter Count")

        # Plot Convergence / stability of graph density in all samples
        fig2.append_trace(pl.graph_objs.Scatter(x=x, y=snapshot_density,
                                                name='Density'), 2, 1)
        fig2.layout.yaxis2.update(title="Graph Density")
        figs.append(fig2)

        # Plot frequency distribution for significant parameter count in
        # samples
        data = []
        hist_data = []
        group_labels = []
        for key in count_dictsplit:
            if np.mean(count_dictsplit[key]) > 0.001:
                hist_data.append(count_dictsplit[key])
                group_labels.append(key)
                data.append(ps.outlined_histogram(count_dictsplit[key], key))

        fig3 = pl.graph_objs.Figure(data=data, layout=ps.baselayout())
        # fig3.layout.update(title="Parameter Count Frequency Distribution",
        #    hovermode="x")
        fig3.layout.xaxis.update(title="Parameter Count")
        fig3.layout.yaxis.update(title="Frequency")
        figs.append(fig3)

        # Plot above plot but with smoothed lines
        fig = ff.create_distplot(hist_data, group_labels, bin_size=1,
                                 show_rug=False, show_hist=False,
                                 curve_type='normal')
        fig.layout.update(font=dict(size=20),
                          xaxis=dict(
            range=[0, 12],
            title="Number of Families"),
            yaxis=dict(title="Fraction of Samples"),
            legend=dict(
            x=0.8,
            y=0.95,
            traceorder='normal',
            bgcolor='#E2E2E2',
            bordercolor='#FFFFFF',
            borderwidth=2
        ))
        figs.append(fig)

        for i in range(len(figs)):
            fig = figs[i]
            title = 'plot{}'.format(i)

            if show:
                pl.offline.plot(fig, filename=title + '.html', **kwargs)
            if save:
                pl.io.write_image(fig, title + '.pdf',
                                  width=imgsize[0], height=imgsize[1])
        if figures is not None:
            figures.extend(figs)


def identify_zero_counts(count_array):
    """Identify indices of often zero counts in an array of counts.

    Whether a parameter count is frequently zero is determined by the test
    implemented in the _is_zero function.

    Parameters
    ----------
    count_array : numpy array
        Parameter count array organized such that count_array[t,i] gives
        i^th parameter count at time t.

    Returns
    -------
    index_array : array
        List of indices in axis 1 of count_array for parameters whose counts
        are frequently zero.
    """
    p_length = len(count_array[0])
    index_array = []
    for i in range(p_length):
        if _is_zero(count_array[:, i]):
            index_array.append(i)
    return(index_array)


def _is_zero(count_array):
    """Determine if an array of counts is frequently zero."""
    if np.mean(count_array) <= 2:
        return(True)
    return(False)
